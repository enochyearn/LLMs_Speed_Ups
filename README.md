# LLMs Speed Ups

This project is a self-learning initiative to implement various techniques that could speed up Large Language Model (LLM) from scratch.

## Table of Contents
1. Decoding Techniques
2. Quantization
3. Batching
4. Paged Attention (Tentative)

## Decoding Techniques
Several decoding techniques will be explored in this project:

1. **Speculative Decoding**
2. **Threshold Decoding**
3. **Staged Speculative Decoding**
4. **Guided Generation**
5. **Lookahead Decoding**
6. **Prompt Lookup Decoding**

## Quantization
The project will also delve into quantization methods:

1. **Simple Quantization by Rounding**
2. **Bitwise Quantization**
3. **Support for GGUF File Format**

## Batching
Batching techniques will be implemented, with a focus on:

1. **Batching**
2. **Continuous Batching**

## Paged Attention
This is a tentative feature that may be included in the future.

Stay tuned for more updates as the project progresses!
